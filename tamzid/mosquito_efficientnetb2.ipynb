{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mosquito Larvae Classification with EfficientNetB2\n",
        "This notebook uses a pretrained EfficientNetB2 model with transfer learning to classify mosquito larvae species (Aedes aegypti vs Culex quinquefasciatus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Data\n",
        "    \"data_path\": \"MLMI-2024-organized\",\n",
        "    \"img_size\": 260,  # EfficientNetB2 uses 260x260\n",
        "    \"batch_size\": 16,  # Smaller batch size for larger model\n",
        "    \"augment\": True,\n",
        "\n",
        "    # Training\n",
        "    \"epochs\": 10,\n",
        "    \"lr\": 1e-4,  # Lower learning rate for fine-tuning\n",
        "    \"optimizer\": \"adam\",  # adam | sgd\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"label_smoothing\": 0.1,\n",
        "\n",
        "    # Model\n",
        "    \"freeze_backbone\": False,  # Set to True to freeze backbone initially\n",
        "    \"dropout\": 0.3,\n",
        "\n",
        "    # Evaluation\n",
        "    \"use_gradcam\": True,\n",
        "    \"output_dir\": \"experiments/efficientnetb2_run_1\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "def get_transforms(cfg, train=True):\n",
        "    \"\"\"\n",
        "    Get transforms for EfficientNetB2\n",
        "    Uses ImageNet normalization\n",
        "    \"\"\"\n",
        "    if train and cfg[\"augment\"]:\n",
        "        t = transforms.Compose([\n",
        "            transforms.Resize((cfg[\"img_size\"], cfg[\"img_size\"])),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomRotation(20),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        t = transforms.Compose([\n",
        "            transforms.Resize((cfg[\"img_size\"], cfg[\"img_size\"])),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    return t\n",
        "\n",
        "def get_dataloaders(cfg):\n",
        "    \"\"\"\n",
        "    Creates DataLoaders for train and test sets using the structure:\n",
        "    dataset/\n",
        "      train/\n",
        "      test/\n",
        "    \"\"\"\n",
        "    train_transform = get_transforms(cfg, train=True)\n",
        "    test_transform = get_transforms(cfg, train=False)\n",
        "\n",
        "    # Use data path from config\n",
        "    base_path = cfg.get(\"data_path\", \"dataset\")\n",
        "    train_dir = os.path.join(base_path, \"train\")\n",
        "    test_dir = os.path.join(base_path, \"test\")\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"WARNING: Train directory not found at {train_dir}\")\n",
        "        print(\"Please ensure 'dataset/train' and 'dataset/test' exist.\")\n",
        "    \n",
        "    try:\n",
        "        train_ds = datasets.ImageFolder(train_dir, train_transform)\n",
        "        test_ds = datasets.ImageFolder(test_dir, test_transform)\n",
        "\n",
        "        print(f\"Train dataset: {len(train_ds)} images\")\n",
        "        print(f\"Test dataset: {len(test_ds)} images\")\n",
        "        print(f\"Classes: {train_ds.classes}\")\n",
        "\n",
        "        train_dl = DataLoader(train_ds, batch_size=cfg[\"batch_size\"], shuffle=True, num_workers=2)\n",
        "        test_dl = DataLoader(test_ds, batch_size=cfg[\"batch_size\"], shuffle=False, num_workers=2)\n",
        "        \n",
        "        return train_dl, test_dl\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading datasets: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Leakage Check\n",
        "Before training, we verify that there are no duplicate or near-duplicate images between train and test sets. This checks for:\n",
        "1. **Exact duplicates** - using MD5 hash\n",
        "2. **Near duplicates** - using perceptual hashing\n",
        "3. **Filename overlaps** - suspicious naming patterns\n",
        "\n",
        "‚ö†Ô∏è **Note**: You'll need to install `imagehash` if not already installed: `pip install imagehash`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import imagehash\n",
        "from PIL import Image\n",
        "\n",
        "def check_data_leakage(data_path, hash_size=8):\n",
        "    \"\"\"\n",
        "    Check for data leakage between train and test sets.\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to dataset root (containing train/ and test/)\n",
        "        hash_size: Size for perceptual hash (default 8)\n",
        "    \n",
        "    Returns:\n",
        "        dict with leakage statistics\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATA LEAKAGE CHECK\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    base_path = Path(data_path)\n",
        "    train_dir = base_path / \"train\"\n",
        "    test_dir = base_path / \"test\"\n",
        "    \n",
        "    if not train_dir.exists() or not test_dir.exists():\n",
        "        print(\"ERROR: Train or test directory not found!\")\n",
        "        return None\n",
        "    \n",
        "    # 1. Check for exact duplicates using MD5 hash\n",
        "    print(\"\\n1. Checking for EXACT duplicates (MD5 hash)...\")\n",
        "    \n",
        "    def compute_md5(file_path):\n",
        "        \"\"\"Compute MD5 hash of file\"\"\"\n",
        "        hash_md5 = hashlib.md5()\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "                hash_md5.update(chunk)\n",
        "        return hash_md5.hexdigest()\n",
        "    \n",
        "    # Get all image files\n",
        "    train_files = list(train_dir.rglob(\"*.jpg\")) + list(train_dir.rglob(\"*.png\"))\n",
        "    test_files = list(test_dir.rglob(\"*.jpg\")) + list(test_dir.rglob(\"*.png\"))\n",
        "    \n",
        "    print(f\"   Train images: {len(train_files)}\")\n",
        "    print(f\"   Test images: {len(test_files)}\")\n",
        "    \n",
        "    # Compute hashes\n",
        "    print(\"   Computing MD5 hashes for train set...\")\n",
        "    train_hashes = {compute_md5(f): f for f in train_files}\n",
        "    \n",
        "    print(\"   Computing MD5 hashes for test set...\")\n",
        "    test_hashes = {compute_md5(f): f for f in test_files}\n",
        "    \n",
        "    # Find exact matches\n",
        "    exact_duplicates = set(train_hashes.keys()) & set(test_hashes.keys())\n",
        "    \n",
        "    if exact_duplicates:\n",
        "        print(f\"\\n   ‚ö†Ô∏è  WARNING: Found {len(exact_duplicates)} EXACT duplicate(s)!\")\n",
        "        for i, hash_val in enumerate(list(exact_duplicates)[:5], 1):\n",
        "            print(f\"      {i}. Train: {train_hashes[hash_val].name}\")\n",
        "            print(f\"         Test:  {test_hashes[hash_val].name}\")\n",
        "        if len(exact_duplicates) > 5:\n",
        "            print(f\"      ... and {len(exact_duplicates) - 5} more\")\n",
        "    else:\n",
        "        print(\"   ‚úì No exact duplicates found!\")\n",
        "    \n",
        "    # 2. Check for near-duplicates using perceptual hash\n",
        "    print(f\"\\n2. Checking for NEAR duplicates (perceptual hash, size={hash_size})...\")\n",
        "    print(\"   This may take a moment...\")\n",
        "    \n",
        "    def compute_perceptual_hash(file_path, hash_size=8):\n",
        "        \"\"\"Compute perceptual hash using average hash\"\"\"\n",
        "        try:\n",
        "            img = Image.open(file_path)\n",
        "            return imagehash.average_hash(img, hash_size=hash_size)\n",
        "        except Exception as e:\n",
        "            print(f\"   Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    # Compute perceptual hashes\n",
        "    train_phashes = {}\n",
        "    for f in train_files:\n",
        "        h = compute_perceptual_hash(f, hash_size)\n",
        "        if h is not None:\n",
        "            train_phashes[f] = h\n",
        "    \n",
        "    test_phashes = {}\n",
        "    for f in test_files:\n",
        "        h = compute_perceptual_hash(f, hash_size)\n",
        "        if h is not None:\n",
        "            test_phashes[f] = h\n",
        "    \n",
        "    # Find similar images (hamming distance <= 5)\n",
        "    similar_pairs = []\n",
        "    threshold = 5  # Hamming distance threshold\n",
        "    \n",
        "    for test_file, test_hash in test_phashes.items():\n",
        "        for train_file, train_hash in train_phashes.items():\n",
        "            distance = test_hash - train_hash\n",
        "            if distance <= threshold:\n",
        "                similar_pairs.append((train_file, test_file, distance))\n",
        "    \n",
        "    if similar_pairs:\n",
        "        print(f\"\\n   ‚ö†Ô∏è  WARNING: Found {len(similar_pairs)} near-duplicate pair(s)!\")\n",
        "        print(f\"   (Hamming distance <= {threshold})\")\n",
        "        for i, (train_f, test_f, dist) in enumerate(similar_pairs[:5], 1):\n",
        "            print(f\"      {i}. Distance={dist}\")\n",
        "            print(f\"         Train: {train_f.name}\")\n",
        "            print(f\"         Test:  {test_f.name}\")\n",
        "        if len(similar_pairs) > 5:\n",
        "            print(f\"      ... and {len(similar_pairs) - 5} more\")\n",
        "    else:\n",
        "        print(f\"   ‚úì No near duplicates found (threshold={threshold})!\")\n",
        "    \n",
        "    # 3. Check filename patterns\n",
        "    print(\"\\n3. Checking for suspicious filename overlaps...\")\n",
        "    train_names = set(f.name for f in train_files)\n",
        "    test_names = set(f.name for f in test_files)\n",
        "    \n",
        "    name_overlaps = train_names & test_names\n",
        "    if name_overlaps:\n",
        "        print(f\"   ‚ö†Ô∏è  WARNING: {len(name_overlaps)} files with same name in train and test!\")\n",
        "        for i, name in enumerate(list(name_overlaps)[:5], 1):\n",
        "            print(f\"      {i}. {name}\")\n",
        "        if len(name_overlaps) > 5:\n",
        "            print(f\"      ... and {len(name_overlaps) - 5} more\")\n",
        "    else:\n",
        "        print(\"   ‚úì No filename overlaps found!\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    leakage_found = exact_duplicates or similar_pairs or name_overlaps\n",
        "    \n",
        "    if leakage_found:\n",
        "        print(\"‚ùå POTENTIAL DATA LEAKAGE DETECTED!\")\n",
        "        print(f\"   - Exact duplicates: {len(exact_duplicates)}\")\n",
        "        print(f\"   - Near duplicates: {len(similar_pairs)}\")\n",
        "        print(f\"   - Filename overlaps: {len(name_overlaps)}\")\n",
        "        print(\"\\n‚ö†Ô∏è  Please review and fix the data split before training!\")\n",
        "    else:\n",
        "        print(\"‚úì NO DATA LEAKAGE DETECTED\")\n",
        "        print(\"  Train and test sets are properly separated.\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return {\n",
        "        \"exact_duplicates\": len(exact_duplicates),\n",
        "        \"near_duplicates\": len(similar_pairs),\n",
        "        \"filename_overlaps\": len(name_overlaps),\n",
        "        \"leakage_detected\": leakage_found\n",
        "    }\n",
        "\n",
        "# Run the data leakage check\n",
        "leakage_results = check_data_leakage(CONFIG[\"data_path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Investigate Near-Duplicates\n",
        "If near-duplicates were detected, use the cells below to investigate them further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import imagehash\n",
        "\n",
        "def visualize_duplicate_pairs(data_path, num_pairs=5, max_distance=5):\n",
        "    \"\"\"\n",
        "    Visualize near-duplicate pairs to manually inspect them.\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to dataset root\n",
        "        num_pairs: Number of pairs to visualize\n",
        "        max_distance: Maximum hamming distance to consider\n",
        "    \"\"\"\n",
        "    base_path = Path(data_path)\n",
        "    train_dir = base_path / \"train\"\n",
        "    test_dir = base_path / \"test\"\n",
        "    \n",
        "    # Get image files\n",
        "    train_files = list(train_dir.rglob(\"*.jpg\")) + list(train_dir.rglob(\"*.png\"))\n",
        "    test_files = list(test_dir.rglob(\"*.jpg\")) + list(test_dir.rglob(\"*.png\"))\n",
        "    \n",
        "    print(f\"Finding near-duplicate pairs (distance <= {max_distance})...\")\n",
        "    \n",
        "    # Compute hashes\n",
        "    train_hashes = {}\n",
        "    for f in train_files[:100]:  # Limit to first 100 for quick check\n",
        "        try:\n",
        "            img = Image.open(f)\n",
        "            train_hashes[f] = imagehash.average_hash(img, hash_size=8)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    test_hashes = {}\n",
        "    for f in test_files[:50]:  # Limit to first 50 for quick check\n",
        "        try:\n",
        "            img = Image.open(f)\n",
        "            test_hashes[f] = imagehash.average_hash(img, hash_size=8)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Find pairs\n",
        "    pairs = []\n",
        "    for test_file, test_hash in test_hashes.items():\n",
        "        for train_file, train_hash in train_hashes.items():\n",
        "            distance = test_hash - train_hash\n",
        "            if distance <= max_distance:\n",
        "                pairs.append((train_file, test_file, distance))\n",
        "                if len(pairs) >= num_pairs:\n",
        "                    break\n",
        "        if len(pairs) >= num_pairs:\n",
        "            break\n",
        "    \n",
        "    # Visualize\n",
        "    if not pairs:\n",
        "        print(\"No pairs found in sample!\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(len(pairs), 2, figsize=(10, 4*len(pairs)))\n",
        "    if len(pairs) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, (train_f, test_f, dist) in enumerate(pairs):\n",
        "        # Load images\n",
        "        train_img = Image.open(train_f)\n",
        "        test_img = Image.open(test_f)\n",
        "        \n",
        "        # Plot train\n",
        "        axes[idx][0].imshow(train_img)\n",
        "        axes[idx][0].set_title(f\"TRAIN: {train_f.parent.name}/{train_f.name}\\nDistance: {dist}\", fontsize=8)\n",
        "        axes[idx][0].axis('off')\n",
        "        \n",
        "        # Plot test\n",
        "        axes[idx][1].imshow(test_img)\n",
        "        axes[idx][1].set_title(f\"TEST: {test_f.parent.name}/{test_f.name}\", fontsize=8)\n",
        "        axes[idx][1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nInspect the pairs above:\")\n",
        "    print(\"- If they look like the SAME mosquito specimen ‚Üí TRUE LEAKAGE\")\n",
        "    print(\"- If they're DIFFERENT mosquitoes but similar ‚Üí FALSE POSITIVE (OK)\")\n",
        "\n",
        "# Visualize some duplicate pairs\n",
        "visualize_duplicate_pairs(CONFIG[\"data_path\"], num_pairs=5, max_distance=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_leakage_statistics(data_path, distance_threshold=5):\n",
        "    \"\"\"\n",
        "    Provide detailed statistics about potential duplicates.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import imagehash\n",
        "    from PIL import Image\n",
        "    from collections import defaultdict\n",
        "    \n",
        "    base_path = Path(data_path)\n",
        "    train_dir = base_path / \"train\"\n",
        "    test_dir = base_path / \"test\"\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"DETAILED LEAKAGE ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Get files\n",
        "    train_files = list(train_dir.rglob(\"*.jpg\")) + list(train_dir.rglob(\"*.png\"))\n",
        "    test_files = list(test_dir.rglob(\"*.jpg\")) + list(test_dir.rglob(\"*.png\"))\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    print(f\"   Train images: {len(train_files)}\")\n",
        "    print(f\"   Test images: {len(test_files)}\")\n",
        "    print(f\"   Max possible pairs: {len(train_files) * len(test_files):,}\")\n",
        "    \n",
        "    # Compute hashes (sample for speed)\n",
        "    print(\"\\nüîç Computing perceptual hashes (sampling)...\")\n",
        "    sample_size = min(200, len(train_files))\n",
        "    train_sample = train_files[:sample_size]\n",
        "    test_sample = test_files[:min(100, len(test_files))]\n",
        "    \n",
        "    train_hashes = {}\n",
        "    for f in train_sample:\n",
        "        try:\n",
        "            img = Image.open(f)\n",
        "            train_hashes[f] = imagehash.average_hash(img, hash_size=8)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    test_hashes = {}\n",
        "    for f in test_sample:\n",
        "        try:\n",
        "            img = Image.open(f)\n",
        "            test_hashes[f] = imagehash.average_hash(img, hash_size=8)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Analyze distances\n",
        "    distances = []\n",
        "    for test_hash in list(test_hashes.values())[:20]:\n",
        "        for train_hash in list(train_hashes.values())[:20]:\n",
        "            distances.append(test_hash - train_hash)\n",
        "    \n",
        "    if distances:\n",
        "        print(f\"\\nüìà Distance Distribution (sample of {len(distances)} pairs):\")\n",
        "        print(f\"   Min distance: {min(distances)}\")\n",
        "        print(f\"   Max distance: {max(distances)}\")\n",
        "        print(f\"   Mean distance: {sum(distances)/len(distances):.2f}\")\n",
        "        print(f\"   Median distance: {sorted(distances)[len(distances)//2]}\")\n",
        "        \n",
        "        # Count by threshold\n",
        "        for thresh in [3, 5, 8, 10]:\n",
        "            count = sum(1 for d in distances if d <= thresh)\n",
        "            pct = (count / len(distances)) * 100\n",
        "            print(f\"   Pairs with distance ‚â§ {thresh:2d}: {count:4d} ({pct:5.1f}%)\")\n",
        "    \n",
        "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
        "    \n",
        "    if len(distances) > 0:\n",
        "        similar_pct = (sum(1 for d in distances if d <= 5) / len(distances)) * 100\n",
        "        \n",
        "        if similar_pct > 50:\n",
        "            print(\"   ‚ö†Ô∏è  HIGH similarity detected (>50% of pairs have distance ‚â§ 5)\")\n",
        "            print(\"   This is COMMON for biological images of the same species!\")\n",
        "            print(\"\\n   Options:\")\n",
        "            print(\"   1. INCREASE threshold to 3 or less for stricter checking\")\n",
        "            print(\"   2. Manually inspect pairs using visualization above\")\n",
        "            print(\"   3. If images are different individuals ‚Üí PROCEED with training\")\n",
        "            print(\"   4. Check if same specimen appears in both sets ‚Üí FIX split\")\n",
        "        elif similar_pct > 20:\n",
        "            print(\"   ‚ö†Ô∏è  MODERATE similarity (20-50% of pairs)\")\n",
        "            print(\"   Recommend manual inspection of flagged pairs\")\n",
        "        else:\n",
        "            print(\"   ‚úì LOW similarity detected (<20% of pairs)\")\n",
        "            print(\"   Most flagged pairs are likely different specimens\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Run detailed analysis\n",
        "analyze_leakage_statistics(CONFIG[\"data_path\"], distance_threshold=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class EfficientNetB2Classifier(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Load pretrained EfficientNetB2\n",
        "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
        "        \n",
        "        # Freeze backbone if specified\n",
        "        if cfg.get(\"freeze_backbone\", False):\n",
        "            for param in self.model.features.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"Backbone frozen - only training classifier head\")\n",
        "        else:\n",
        "            print(\"Training full model (backbone + classifier)\")\n",
        "        \n",
        "        # Get the number of input features for the classifier\n",
        "        in_features = self.model.classifier[1].in_features\n",
        "        \n",
        "        # Replace classifier with custom head\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=cfg[\"dropout\"], inplace=True),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def unfreeze_backbone(self):\n",
        "        \"\"\"Unfreeze the backbone for fine-tuning\"\"\"\n",
        "        for param in self.model.features.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"Backbone unfrozen - now training full model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_score = [], []\n",
        "    y_pred = []\n",
        "\n",
        "    print(\"\\nRunning Evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            # Forward pass\n",
        "            logits = model(x)\n",
        "            # Get probabilities\n",
        "            probs = logits.softmax(1)[:, 1].cpu()\n",
        "            # Get hard predictions\n",
        "            preds = logits.argmax(1).cpu()\n",
        "\n",
        "            y_true.extend(y.numpy())\n",
        "            y_score.extend(probs.numpy())\n",
        "            y_pred.extend(preds.numpy())\n",
        "\n",
        "    # Check if we have gathered any data\n",
        "    if not y_true:\n",
        "        print(\"No evaluation data found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_score)\n",
        "        print(f\"ROC-AUC: {auc:.4f}\")\n",
        "    except ValueError:\n",
        "        print(\"ROC-AUC: N/A (Only one class present in targets)\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "def plot_training_curves(train_losses, train_accs, output_dir):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    \n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy', color='orange')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'training_curves.png'), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, output_dir):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curve(y_true, y_scores, output_dir):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join(output_dir, 'roc_curve.png'), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def apply_gradcam(model, image_tensor, target_class=None):\n",
        "    \"\"\"\n",
        "    Generates GradCAM heatmap for EfficientNetB2.\n",
        "    image_tensor: (1, C, H, W)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Hook to capture gradients and activations\n",
        "    gradients = []\n",
        "    activations = []\n",
        "    \n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "        \n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output)\n",
        "        \n",
        "    # Hook the last convolutional layer in EfficientNet features\n",
        "    # For EfficientNetB2, the features are in model.model.features\n",
        "    target_layer = model.model.features[-1]\n",
        "    \n",
        "    handle_f = target_layer.register_forward_hook(forward_hook)\n",
        "    handle_b = target_layer.register_full_backward_hook(backward_hook)\n",
        "    \n",
        "    # Forward\n",
        "    output = model(image_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax(dim=1).item()\n",
        "        \n",
        "    # Backward\n",
        "    model.zero_grad()\n",
        "    score = output[0, target_class]\n",
        "    score.backward()\n",
        "    \n",
        "    # Get captures\n",
        "    grads = gradients[0].cpu().data.numpy()[0] # (C, H, W)\n",
        "    fmaps = activations[0].cpu().data.numpy()[0] # (C, H, W)\n",
        "    \n",
        "    handle_f.remove()\n",
        "    handle_b.remove()\n",
        "    \n",
        "    # Weights = Global Average Pooling of Gradients\n",
        "    weights = np.mean(grads, axis=(1, 2)) # (C,)\n",
        "    \n",
        "    # Weighted combination of feature maps\n",
        "    cam = np.zeros(fmaps.shape[1:], dtype=np.float32) # (H, W)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * fmaps[i]\n",
        "        \n",
        "    # ReLU\n",
        "    cam = np.maximum(cam, 0)\n",
        "    \n",
        "    # Normalize\n",
        "    if np.max(cam) > 0:\n",
        "        cam = cam / np.max(cam)\n",
        "    else:\n",
        "        cam = cam # avoid div by zero\n",
        "        \n",
        "    return cam\n",
        "\n",
        "def denormalize_image(img_tensor):\n",
        "    \"\"\"Denormalize ImageNet normalized image\"\"\"\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    \n",
        "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "def save_gradcam_image(image_tensor, cam, output_path, label_text):\n",
        "    # Denormalize image for visualization\n",
        "    img = denormalize_image(image_tensor)\n",
        "    img = np.uint8(255 * img)\n",
        "    \n",
        "    # Resize cam to image size\n",
        "    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    \n",
        "    # Superimpose\n",
        "    superimposed_img = heatmap * 0.4 + img * 0.6\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "    \n",
        "    # Add label\n",
        "    cv2.putText(superimposed_img, label_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "    \n",
        "    cv2.imwrite(output_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, SGD\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "def get_optimizer(cfg, model):\n",
        "    if cfg[\"optimizer\"] == \"adam\":\n",
        "        print(f\"Using Adam optimizer with lr={cfg['lr']}, weight_decay={cfg['weight_decay']}\")\n",
        "        return Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
        "    print(f\"Using SGD optimizer with lr={cfg['lr']}, momentum=0.9, weight_decay={cfg['weight_decay']}\")\n",
        "    return SGD(model.parameters(), lr=cfg[\"lr\"], momentum=0.9, weight_decay=cfg[\"weight_decay\"])\n",
        "\n",
        "def train_model():\n",
        "    # 0. Setup Output Dir\n",
        "    os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "    print(f\"Outputs will be saved in: {CONFIG['output_dir']}\")\n",
        "\n",
        "    # 1. Setup\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Running on: {DEVICE}\")\n",
        "    \n",
        "    # 2. Data\n",
        "    print(\"Loading data...\")\n",
        "    train_dl, test_dl = get_dataloaders(CONFIG)\n",
        "    \n",
        "    if train_dl is None or test_dl is None:\n",
        "        print(\"Data loading failed.\")\n",
        "        return\n",
        "\n",
        "    # 3. Model\n",
        "    print(\"Initializing EfficientNetB2 model...\")\n",
        "    model = EfficientNetB2Classifier(CONFIG, num_classes=2).to(DEVICE)\n",
        "    \n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    \n",
        "    # 4. Optimization\n",
        "    optimizer = get_optimizer(CONFIG, model)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG[\"label_smoothing\"])\n",
        "    \n",
        "    # Trackers\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    \n",
        "    # 5. Training Loop\n",
        "    print(f\"\\nStarting training for {CONFIG['epochs']} epochs...\")\n",
        "    print(\"=\"*60)\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        model.train()\n",
        "        correct, total, loss_sum = 0, 0, 0\n",
        "        \n",
        "        for i, (x, y) in enumerate(train_dl):\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_sum += loss.item()\n",
        "            correct += (out.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            \n",
        "        # Stats\n",
        "        epoch_acc = correct / total if total > 0 else 0\n",
        "        avg_loss = loss_sum / len(train_dl) if len(train_dl) > 0 else 0\n",
        "        \n",
        "        train_losses.append(avg_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "        \n",
        "        print(f\"[Epoch {epoch+1:02d}/{CONFIG['epochs']}] Loss={avg_loss:.4f} Acc={epoch_acc:.4f}\")\n",
        "        \n",
        "    total_time = time.time() - start_time\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Training finished in {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "    \n",
        "    # Save training curves\n",
        "    print(\"\\nGenerating training curves...\")\n",
        "    plot_training_curves(train_losses, train_accs, CONFIG[\"output_dir\"])\n",
        "    \n",
        "    # 6. Evaluation\n",
        "    print(\"\\nEvaluating on Test Set...\")\n",
        "    model.eval()\n",
        "    y_true, y_score, y_pred = [], [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y in test_dl:\n",
        "            x = x.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            probs = logits.softmax(1)[:, 1].cpu()\n",
        "            preds = logits.argmax(1).cpu()\n",
        "\n",
        "            y_true.extend(y.numpy())\n",
        "            y_score.extend(probs.numpy())\n",
        "            y_pred.extend(preds.numpy())\n",
        "            \n",
        "    # Save Model\n",
        "    save_path = os.path.join(CONFIG[\"output_dir\"], \"efficientnetb2_mosq.pth\")\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"\\nModel saved to {save_path}\")\n",
        "    \n",
        "    # Generate Plots\n",
        "    print(\"Generating ROC Curve...\")\n",
        "    plot_roc_curve(y_true, y_score, CONFIG[\"output_dir\"])\n",
        "    \n",
        "    print(\"Generating Confusion Matrix...\")\n",
        "    classes = [\"Aedes aegypti\", \"Culex quinquefasciatus\"]\n",
        "    plot_confusion_matrix(y_true, y_pred, classes, CONFIG[\"output_dir\"])\n",
        "    \n",
        "    # 7. GradCAM Generation\n",
        "    if CONFIG[\"use_gradcam\"]:\n",
        "        print(\"\\nGenerating GradCAM samples...\")\n",
        "        model.eval()\n",
        "        gradcam_dir = os.path.join(CONFIG[\"output_dir\"], \"gradcam\")\n",
        "        os.makedirs(gradcam_dir, exist_ok=True)\n",
        "        \n",
        "        # Pick 5 random images from test set\n",
        "        inputs, labels = next(iter(test_dl))\n",
        "        indices = random.sample(range(len(inputs)), min(5, len(inputs)))\n",
        "        \n",
        "        for idx in indices:\n",
        "            img_tensor = inputs[idx].unsqueeze(0).to(DEVICE) # (1, 3, H, W)\n",
        "            label = labels[idx].item()\n",
        "            class_name = classes[label]\n",
        "            \n",
        "            # Predict\n",
        "            out = model(img_tensor)\n",
        "            pred = out.argmax(1).item()\n",
        "            pred_name = classes[pred]\n",
        "            \n",
        "            # Generate CAM\n",
        "            cam = apply_gradcam(model, img_tensor, target_class=pred)\n",
        "            \n",
        "            # Save\n",
        "            fname = f\"cam_{idx}_true_{class_name}_pred_{pred_name}.jpg\"\n",
        "            save_path = os.path.join(gradcam_dir, fname)\n",
        "            label_text = f\"True: {class_name}, Pred: {pred_name}\"\n",
        "            save_gradcam_image(img_tensor[0], cam, save_path, label_text)\n",
        "            print(f\"  Saved {fname}\")\n",
        "\n",
        "    # Text Evaluation\n",
        "    evaluate(model, test_dl, DEVICE)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training and Evaluation Complete!\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Training\n",
        "Execute the cell below to start the training process with EfficientNetB2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outputs will be saved in: experiments/efficientnetb2_run_1\n",
            "Running on: cuda\n",
            "Loading data...\n",
            "Train dataset: 560 images\n",
            "Test dataset: 240 images\n",
            "Classes: ['Aedes aegypti', 'Culex quinquefasciatus']\n",
            "Initializing EfficientNetB2 model...\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to C:\\Users\\tamji/.cache\\torch\\hub\\checkpoints\\efficientnet_b2_rwightman-c35c1473.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35.2M/35.2M [00:03<00:00, 9.63MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training full model (backbone + classifier)\n",
            "Total parameters: 7,703,812\n",
            "Trainable parameters: 7,703,812\n",
            "Using Adam optimizer with lr=0.0001, weight_decay=0.0001\n",
            "\n",
            "Starting training for 10 epochs...\n",
            "============================================================\n",
            "[Epoch 01/10] Loss=0.5480 Acc=0.7893\n",
            "[Epoch 02/10] Loss=0.3201 Acc=0.9393\n",
            "[Epoch 03/10] Loss=0.2925 Acc=0.9607\n",
            "[Epoch 04/10] Loss=0.2561 Acc=0.9786\n",
            "[Epoch 05/10] Loss=0.2452 Acc=0.9786\n",
            "[Epoch 06/10] Loss=0.2482 Acc=0.9786\n",
            "[Epoch 07/10] Loss=0.2304 Acc=0.9929\n",
            "[Epoch 08/10] Loss=0.2297 Acc=0.9911\n",
            "[Epoch 09/10] Loss=0.2223 Acc=0.9964\n",
            "[Epoch 10/10] Loss=0.2235 Acc=0.9911\n",
            "============================================================\n",
            "Training finished in 741.64s (12.36 minutes)\n",
            "\n",
            "Generating training curves...\n",
            "\n",
            "Evaluating on Test Set...\n",
            "\n",
            "Model saved to experiments/efficientnetb2_run_1\\efficientnetb2_mosq.pth\n",
            "Generating ROC Curve...\n",
            "Generating Confusion Matrix...\n",
            "\n",
            "Generating GradCAM samples...\n",
            "  Saved cam_10_true_Aedes aegypti_pred_Aedes aegypti.jpg\n",
            "  Saved cam_7_true_Aedes aegypti_pred_Aedes aegypti.jpg\n",
            "  Saved cam_8_true_Aedes aegypti_pred_Aedes aegypti.jpg\n",
            "  Saved cam_11_true_Aedes aegypti_pred_Aedes aegypti.jpg\n",
            "  Saved cam_15_true_Aedes aegypti_pred_Aedes aegypti.jpg\n",
            "\n",
            "Running Evaluation...\n",
            "ROC-AUC: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000       120\n",
            "           1     1.0000    1.0000    1.0000       120\n",
            "\n",
            "    accuracy                         1.0000       240\n",
            "   macro avg     1.0000    1.0000    1.0000       240\n",
            "weighted avg     1.0000    1.0000    1.0000       240\n",
            "\n",
            "\n",
            "============================================================\n",
            "Training and Evaluation Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Run the training\n",
        "train_model()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
